# âš¡ **Module 5 â€“ Resource Efficiency & Scheduling in Kubernetes**

---

## ğŸš€ 1. Introduction

Kubernetes schedules pods automatically, but in production:

* Some workloads need to run on **specific nodes** (e.g., GPU, SSD).
* Some workloads must **not run together** (e.g., master & worker separation).
* Some workloads are **more critical** than others (Priority Classes).
* Clusters need to remain balanced (Topology Spread, Descheduler).

This module covers:

* NodeSelectors, Affinity/Anti-Affinity.
* Taints & Tolerations.
* Pod Disruption Budgets (PDB).
* Quality of Service (QoS) for Pods.
* Priority Classes.
* Topology Spread Constraints.
* Descheduler for rebalancing workloads.

---

## ğŸ—‚ï¸ 2. Scheduling Mechanisms

### ğŸ“– Concepts

* **NodeSelector** â†’ Simple key-value scheduling.
* **Node Affinity** â†’ Advanced rules for pod placement.
* **Pod Affinity/Anti-Affinity** â†’ Schedule pods together or apart.
* **Taints & Tolerations** â†’ Prevent unwanted scheduling.
* **Topology Spread Constraints** â†’ Even distribution across nodes/zones.
* **Pod Disruption Budgets (PDBs)** â†’ Protect against voluntary disruptions.
* **Descheduler** â†’ Moves pods when nodes become unbalanced.

---

## ğŸ§ª 3. Hands-on Labs

### ğŸ”¹ Lab 1: NodeSelector

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-node-selector
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    disktype: ssd
```

ğŸ‘‰ Label your node:

```bash
kubectl label nodes worker-1 disktype=ssd
```

---

### ğŸ”¹ Lab 2: Node Affinity

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: disktype
          operator: In
          values:
          - ssd
```

---

### ğŸ”¹ Lab 3: Pod Affinity & Anti-Affinity

**Pod Affinity** â€“ Schedule frontend near backend:

```yaml
podAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchExpressions:
      - key: app
        operator: In
        values:
        - backend
    topologyKey: "kubernetes.io/hostname"
```

**Pod Anti-Affinity** â€“ Avoid placing two same apps on same node:

```yaml
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchExpressions:
      - key: app
        operator: In
        values:
        - frontend
    topologyKey: "kubernetes.io/hostname"
```

---

### ğŸ”¹ Lab 4: Taints & Tolerations

ğŸ‘‰ Taint a node:

```bash
kubectl taint nodes worker-2 dedicated=critical:NoSchedule
```

ğŸ‘‰ Pod toleration:

```yaml
tolerations:
- key: "dedicated"
  operator: "Equal"
  value: "critical"
  effect: "NoSchedule"
```

---

### ğŸ”¹ Lab 5: Pod Disruption Budget (PDB)

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: backend-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: backend
```

ğŸ‘‰ Ensures at least 2 pods are always running during upgrades.

---

### ğŸ”¹ Lab 6: QoS (Quality of Service)

* **BestEffort** â†’ No requests/limits.
* **Burstable** â†’ Requests < Limits.
* **Guaranteed** â†’ Requests = Limits.

```yaml
resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "500m"
    memory: "1Gi"
```

---

### ğŸ”¹ Lab 7: Priority Classes

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000
globalDefault: false
description: "This class is for critical workloads"
```

Apply to Pod:

```yaml
priorityClassName: high-priority
```

---

### ğŸ”¹ Lab 8: Topology Spread Constraints

```yaml
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: kubernetes.io/hostname
  whenUnsatisfiable: DoNotSchedule
  labelSelector:
    matchLabels:
      app: backend
```

ğŸ‘‰ Ensures even distribution across nodes.

---

### ğŸ”¹ Lab 9: Descheduler

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/descheduler/master/kubernetes/base/rbac.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/descheduler/master/kubernetes/base/deployment.yaml
```

ğŸ‘‰ Run strategies: `RemoveDuplicates`, `LowNodeUtilization`, `RemovePodsViolatingAffinity`.

---

## ğŸš€ 4. Production Best Practices

* Always set **resource requests/limits** â†’ avoids node overcommit.
* Use **Affinity + Anti-Affinity** to enforce HA.
* Reserve dedicated nodes using **taints/tolerations** for critical apps.
* Apply **PDBs** during cluster upgrades.
* Use **PriorityClasses** for tier-1 services.
* Enable **Descheduler** in large clusters for rebalancing.

---

## ğŸ¯ 5. Interview & Discussion Questions

1. Difference between **NodeSelector** and **Node Affinity**?
2. When would you use **Pod Anti-Affinity**?
3. Whatâ€™s the purpose of **Taints & Tolerations**?
4. Explain **QoS classes** in Kubernetes.
5. How would you ensure critical workloads survive **cluster upgrades**?
6. What is the role of the **Descheduler**?

---

## âœ… Summary of Module 5

By completing this module, you now know how to:

* Efficiently schedule workloads with NodeSelector, Affinity, Taints.
* Protect apps using PDBs.
* Prioritize workloads with QoS & Priority Classes.
* Balance workloads with Topology Spread & Descheduler.

ğŸ“Œ **Outcome**: You can now optimize **resource efficiency & workload placement** in production Kubernetes clusters.

---

ğŸ‘‰ Do you want me to continue with **Module 6 â€“ Ingress & Load Balancing Strategies** (Ingress Controllers, Traffic Routing, LB Options, Gateway API) in the same professional format?


â€”--------------------&---------------------------

Great ğŸš€ Letâ€™s move on to **Module 6 â€“ Ingress & Load Balancing Strategies in Kubernetes**.

---

# ğŸŒ **Module 6 â€“ Ingress & Load Balancing Strategies**

---

## ğŸš€ 1. Introduction

In Kubernetes, **Services** expose workloads, but for production:

* We need **advanced traffic management** (TLS, routing rules, rewrites).
* We need **load balancing** across multiple pods/nodes.
* We need **secure ingress** from the outside world.

This module covers:

* Load Balancing Options.
* Ingress Controllers.
* Kubernetes Ingress Resources.
* Gateway API (next-gen Ingress).
* Real-world strategies for scaling & traffic routing.

---

## ğŸ—‚ï¸ 2. Load Balancing Options

### ğŸ“– Concepts

1. **ClusterIP** â†’ Internal LB.
2. **NodePort** â†’ Exposes service on every node.
3. **LoadBalancer** â†’ Cloud providerâ€™s LB (AWS ELB, Azure LB, GCP LB).
4. **Ingress** â†’ Layer 7 (HTTP/HTTPS) routing.
5. **HAProxy / NGINX / Traefik** â†’ Software LBs inside cluster.
6. **MetalLB** â†’ LoadBalancer for bare-metal clusters.
7. **Gateway API** â†’ New Kubernetes-native load balancing & routing framework.

### ğŸš€ Best Practices

* Use **Ingress or Gateway API** instead of NodePort.
* For on-prem â†’ use **MetalLB** or HAProxy.
* For multi-cluster â†’ prefer **Gateway API**.

---

## ğŸ§ª 3. Hands-on Labs

### ğŸ”¹ Lab 1: Installing NGINX Ingress Controller

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
```

Verify:

```bash
kubectl get pods -n ingress-nginx
```

---

### ğŸ”¹ Lab 2: Basic Ingress Resource

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
spec:
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

---

### ğŸ”¹ Lab 3: Ingress with TLS

```yaml
spec:
  tls:
  - hosts:
    - web.example.com
    secretName: web-tls
```

ğŸ‘‰ Use **cert-manager + Letâ€™s Encrypt** to automate TLS.

---

### ğŸ”¹ Lab 4: Path-Based Routing

```yaml
rules:
- host: app.example.com
  http:
    paths:
    - path: /frontend
      pathType: Prefix
      backend:
        service:
          name: frontend
          port:
            number: 80
    - path: /backend
      pathType: Prefix
      backend:
        service:
          name: backend
          port:
            number: 80
```

---

### ğŸ”¹ Lab 5: Weighted Traffic (Canary Release)

```yaml
nginx.ingress.kubernetes.io/canary: "true"
nginx.ingress.kubernetes.io/canary-weight: "20"
```

ğŸ‘‰ Sends 20% traffic to canary deployment.

---

### ğŸ”¹ Lab 6: MetalLB for Bare-Metal

```bash
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.10/config/manifests/metallb-native.yaml
```

ğŸ‘‰ Configure IP address pool for LoadBalancer service.

---

### ğŸ”¹ Lab 7: Gateway API (Future of Ingress)

Install Gateway CRDs:

```bash
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml
```

Example Gateway:

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: my-gateway
spec:
  gatewayClassName: nginx
  listeners:
  - name: http
    port: 80
    protocol: HTTP
```

---

## ğŸš€ 4. Advanced Production Strategies

* Use **TLS everywhere** (no plain HTTP in prod).
* Use **WAF (Web Application Firewall)** with Ingress (NGINX, AWS WAF).
* Enable **rate limiting & DDoS protection**.
* Use **sticky sessions** if workloads are session-dependent.
* For **multi-cluster traffic routing** â†’ use Service Mesh + Gateway API.

---

## ğŸ¯ 5. Interview & Discussion Questions

1. Difference between **NodePort, LoadBalancer, and Ingress**?
2. Why do we need an **Ingress Controller** instead of just Services?
3. How would you handle **canary deployments** with Ingress?
4. Whatâ€™s the difference between **Ingress** and **Gateway API**?
5. How would you expose a service in **bare-metal Kubernetes**?

---

## âœ… Summary of Module 6

By completing this module, you now know how to:

* Expose apps using LoadBalancer & Ingress.
* Secure workloads with TLS.
* Route traffic using path-based & host-based rules.
* Use MetalLB for bare-metal.
* Adopt Gateway API for modern traffic management.

ğŸ“Œ **Outcome**: You can now design **scalable & secure ingress and load balancing strategies** for Kubernetes in production.

