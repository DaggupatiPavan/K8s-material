# ⚡ **Module 5 – Resource Efficiency & Scheduling in Kubernetes**

---

## 🚀 1. Introduction

Kubernetes schedules pods automatically, but in production:

* Some workloads need to run on **specific nodes** (e.g., GPU, SSD).
* Some workloads must **not run together** (e.g., master & worker separation).
* Some workloads are **more critical** than others (Priority Classes).
* Clusters need to remain balanced (Topology Spread, Descheduler).

This module covers:

* NodeSelectors, Affinity/Anti-Affinity.
* Taints & Tolerations.
* Pod Disruption Budgets (PDB).
* Quality of Service (QoS) for Pods.
* Priority Classes.
* Topology Spread Constraints.
* Descheduler for rebalancing workloads.

---

## 🗂️ 2. Scheduling Mechanisms

### 📖 Concepts

* **NodeSelector** → Simple key-value scheduling.
* **Node Affinity** → Advanced rules for pod placement.
* **Pod Affinity/Anti-Affinity** → Schedule pods together or apart.
* **Taints & Tolerations** → Prevent unwanted scheduling.
* **Topology Spread Constraints** → Even distribution across nodes/zones.
* **Pod Disruption Budgets (PDBs)** → Protect against voluntary disruptions.
* **Descheduler** → Moves pods when nodes become unbalanced.

---

## 🧪 3. Hands-on Labs

### 🔹 Lab 1: NodeSelector

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-node-selector
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    disktype: ssd
```

👉 Label your node:

```bash
kubectl label nodes worker-1 disktype=ssd
```

---

### 🔹 Lab 2: Node Affinity

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: disktype
          operator: In
          values:
          - ssd
```

---

### 🔹 Lab 3: Pod Affinity & Anti-Affinity

**Pod Affinity** – Schedule frontend near backend:

```yaml
podAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchExpressions:
      - key: app
        operator: In
        values:
        - backend
    topologyKey: "kubernetes.io/hostname"
```

**Pod Anti-Affinity** – Avoid placing two same apps on same node:

```yaml
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchExpressions:
      - key: app
        operator: In
        values:
        - frontend
    topologyKey: "kubernetes.io/hostname"
```

---

### 🔹 Lab 4: Taints & Tolerations

👉 Taint a node:

```bash
kubectl taint nodes worker-2 dedicated=critical:NoSchedule
```

👉 Pod toleration:

```yaml
tolerations:
- key: "dedicated"
  operator: "Equal"
  value: "critical"
  effect: "NoSchedule"
```

---

### 🔹 Lab 5: Pod Disruption Budget (PDB)

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: backend-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: backend
```

👉 Ensures at least 2 pods are always running during upgrades.

---

### 🔹 Lab 6: QoS (Quality of Service)

* **BestEffort** → No requests/limits.
* **Burstable** → Requests < Limits.
* **Guaranteed** → Requests = Limits.

```yaml
resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "500m"
    memory: "1Gi"
```

---

### 🔹 Lab 7: Priority Classes

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000
globalDefault: false
description: "This class is for critical workloads"
```

Apply to Pod:

```yaml
priorityClassName: high-priority
```

---

### 🔹 Lab 8: Topology Spread Constraints

```yaml
topologySpreadConstraints:
- maxSkew: 1
  topologyKey: kubernetes.io/hostname
  whenUnsatisfiable: DoNotSchedule
  labelSelector:
    matchLabels:
      app: backend
```

👉 Ensures even distribution across nodes.

---

### 🔹 Lab 9: Descheduler

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/descheduler/master/kubernetes/base/rbac.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/descheduler/master/kubernetes/base/deployment.yaml
```

👉 Run strategies: `RemoveDuplicates`, `LowNodeUtilization`, `RemovePodsViolatingAffinity`.

---

## 🚀 4. Production Best Practices

* Always set **resource requests/limits** → avoids node overcommit.
* Use **Affinity + Anti-Affinity** to enforce HA.
* Reserve dedicated nodes using **taints/tolerations** for critical apps.
* Apply **PDBs** during cluster upgrades.
* Use **PriorityClasses** for tier-1 services.
* Enable **Descheduler** in large clusters for rebalancing.

---

## 🎯 5. Interview & Discussion Questions

1. Difference between **NodeSelector** and **Node Affinity**?
2. When would you use **Pod Anti-Affinity**?
3. What’s the purpose of **Taints & Tolerations**?
4. Explain **QoS classes** in Kubernetes.
5. How would you ensure critical workloads survive **cluster upgrades**?
6. What is the role of the **Descheduler**?

---

## ✅ Summary of Module 5

By completing this module, you now know how to:

* Efficiently schedule workloads with NodeSelector, Affinity, Taints.
* Protect apps using PDBs.
* Prioritize workloads with QoS & Priority Classes.
* Balance workloads with Topology Spread & Descheduler.

📌 **Outcome**: You can now optimize **resource efficiency & workload placement** in production Kubernetes clusters.

---

👉 Do you want me to continue with **Module 6 – Ingress & Load Balancing Strategies** (Ingress Controllers, Traffic Routing, LB Options, Gateway API) in the same professional format?


—--------------------&---------------------------

Great 🚀 Let’s move on to **Module 6 – Ingress & Load Balancing Strategies in Kubernetes**.

---

# 🌐 **Module 6 – Ingress & Load Balancing Strategies**

---

## 🚀 1. Introduction

In Kubernetes, **Services** expose workloads, but for production:

* We need **advanced traffic management** (TLS, routing rules, rewrites).
* We need **load balancing** across multiple pods/nodes.
* We need **secure ingress** from the outside world.

This module covers:

* Load Balancing Options.
* Ingress Controllers.
* Kubernetes Ingress Resources.
* Gateway API (next-gen Ingress).
* Real-world strategies for scaling & traffic routing.

---

## 🗂️ 2. Load Balancing Options

### 📖 Concepts

1. **ClusterIP** → Internal LB.
2. **NodePort** → Exposes service on every node.
3. **LoadBalancer** → Cloud provider’s LB (AWS ELB, Azure LB, GCP LB).
4. **Ingress** → Layer 7 (HTTP/HTTPS) routing.
5. **HAProxy / NGINX / Traefik** → Software LBs inside cluster.
6. **MetalLB** → LoadBalancer for bare-metal clusters.
7. **Gateway API** → New Kubernetes-native load balancing & routing framework.

### 🚀 Best Practices

* Use **Ingress or Gateway API** instead of NodePort.
* For on-prem → use **MetalLB** or HAProxy.
* For multi-cluster → prefer **Gateway API**.

---

## 🧪 3. Hands-on Labs

### 🔹 Lab 1: Installing NGINX Ingress Controller

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
```

Verify:

```bash
kubectl get pods -n ingress-nginx
```

---

### 🔹 Lab 2: Basic Ingress Resource

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
spec:
  rules:
  - host: web.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

---

### 🔹 Lab 3: Ingress with TLS

```yaml
spec:
  tls:
  - hosts:
    - web.example.com
    secretName: web-tls
```

👉 Use **cert-manager + Let’s Encrypt** to automate TLS.

---

### 🔹 Lab 4: Path-Based Routing

```yaml
rules:
- host: app.example.com
  http:
    paths:
    - path: /frontend
      pathType: Prefix
      backend:
        service:
          name: frontend
          port:
            number: 80
    - path: /backend
      pathType: Prefix
      backend:
        service:
          name: backend
          port:
            number: 80
```

---

### 🔹 Lab 5: Weighted Traffic (Canary Release)

```yaml
nginx.ingress.kubernetes.io/canary: "true"
nginx.ingress.kubernetes.io/canary-weight: "20"
```

👉 Sends 20% traffic to canary deployment.

---

### 🔹 Lab 6: MetalLB for Bare-Metal

```bash
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.10/config/manifests/metallb-native.yaml
```

👉 Configure IP address pool for LoadBalancer service.

---

### 🔹 Lab 7: Gateway API (Future of Ingress)

Install Gateway CRDs:

```bash
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml
```

Example Gateway:

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: my-gateway
spec:
  gatewayClassName: nginx
  listeners:
  - name: http
    port: 80
    protocol: HTTP
```

---

## 🚀 4. Advanced Production Strategies

* Use **TLS everywhere** (no plain HTTP in prod).
* Use **WAF (Web Application Firewall)** with Ingress (NGINX, AWS WAF).
* Enable **rate limiting & DDoS protection**.
* Use **sticky sessions** if workloads are session-dependent.
* For **multi-cluster traffic routing** → use Service Mesh + Gateway API.

---

## 🎯 5. Interview & Discussion Questions

1. Difference between **NodePort, LoadBalancer, and Ingress**?
2. Why do we need an **Ingress Controller** instead of just Services?
3. How would you handle **canary deployments** with Ingress?
4. What’s the difference between **Ingress** and **Gateway API**?
5. How would you expose a service in **bare-metal Kubernetes**?

---

## ✅ Summary of Module 6

By completing this module, you now know how to:

* Expose apps using LoadBalancer & Ingress.
* Secure workloads with TLS.
* Route traffic using path-based & host-based rules.
* Use MetalLB for bare-metal.
* Adopt Gateway API for modern traffic management.

📌 **Outcome**: You can now design **scalable & secure ingress and load balancing strategies** for Kubernetes in production.

